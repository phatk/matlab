Matlab selected codes from MS&E 211X

% Explored Gradient Method
clc
clear

a1 = [1;0;1];
a2 = [0;1;1];
a3 = [-1;0;1];
a4 = [0;-1;1];

iterations = 1000;
alpha = 0.1;

x = zeros(3, iterations);
x(1,:) = 0;
eu_norm = zeros(1, iterations);
neg_log = zeros(1, iterations);

for K=1:iterations
    gradient = exp(-a1' * x(:,K)) / (1 + exp(-a1' * x(:,K))) * (-a1) + ...
        exp(-a2' * x(:,K)) / (1 + exp(-a2' * x(:,K))) * (-a2) + ...
        exp(a3' * x(:,K)) / (1 + exp(a3' * x(:,K))) * (a3) + ...
        exp(a4' * x(:,K)) / (1 + exp(a4' * x(:,K))) * (a4);
    x(:,K+1) = x(:,K) - alpha * gradient;
    eu_norm(K) = norm(x(:,K));
    neg_log(K) = log(1 + exp(-a1' * x(:,K))) + log(1 + exp(-a2' * x(:,K))) + ...
        1 + exp(a3' * x(:,K)) + 1 + exp(a4' * x(:,K));
    if (norm(gradient) < 0.001)
        break;
    end 
end

figure(1)
plot(eu_norm(1:iterations));
title({'Gradient Descent with step size 0.1 and x_0 as 0'});
ylabel('Euclidean Norm')

figure(2)
plot(neg_log(1:iterations));
title({'Gradient Descent with step size 0.1 and x_0 as 0'});
ylabel('Value of Neg log-likelihood')















% Applied 3-block ADMM to solve Fisher Price Social Optimization

clc
clear

% Set constraint matrix for the first block variable x(1:4)
A1=[1 1 0 0 1 0; 0 0 1 1 0 1; 2 0 1 0 0 0; 0 3 0 1 0 0; 
    0 0 0 0 1 1;eye(6)];
% Set constraint matrix for the second block including three u
A2=[zeros(2,3);-eye(3);zeros(6,3)];
A3=[zeros(5,6);-eye(6)];

% Precompute the inverse (A1'*A1) that would be repeatdly used later.
A1INV=inv(A1'*A1);
% Set the b vector in dimension 8
b=[1;1;zeros(9,1)];
% set the budgets of the two agent
w=[5;8;2];
beta=1;

% Set the initial solution where the two goosd are evenly distributed
x=[1/3;1/3;1/3;1/3;1/3;1/3;
   1;4/3;2/3;
   1/3;1/3;1/3;1/3;1/3;1/3];
% Set the multipliers to zeros
y=zeros(11,1);
% Track eu_norm
eu_norm = zeros(1, 100);

for k=1:100;   
 %Update the first block variable:
 %   prepare the fixed coefficients
 b1=b-A2*x(7:9)-A3*x(10:15);
 %   minimize a quadratic objective
 x(1:6)=A1INV*(A1'*(b1+y/beta));
 
 %Update the second block variable: First three u and later six s variables
 %  prepare the fixed coefficientts 
 b2=A1*x(1:6)-b;
 tempb=beta*b2(3)-y(3);
 %  find the right root for u1
 x(7)=(tempb+sqrt(tempb^2+4*beta*w(1)))/(2*beta);
 %  find the right root for u2
 tempb=beta*b2(4)-y(4);
 x(8)=(tempb+sqrt(tempb^2+4*beta*w(2)))/(2*beta);
 tempb=beta*b2(5)-y(5);
 x(9)=(tempb+sqrt(tempb^2+4*beta*w(3)))/(2*beta);
 
 %  Update the six s variables that need to be nonnegative
 x(10:15)=max(0,b2(6:11)-y(6:11)/beta);
 
 % Update the multipliers
 y=y-beta*(A1*x(1:6)+A2*x(7:9)+A3*x(10:15)-b);
 l2_norm(k) = norm(A1*x(1:6)+A2*x(7:9)+A3*x(10:15)-b);
end;

figure(1)
plot(l2_norm(1:100));
title({'l2 norm of error'});
xlabel('iteration')
ylabel('l2 norm')







% Solved using Regularized SVM

clc
clear

delta = 3;
x0 = 0.0001;
x1 = 0.0001;
x2 = 0;
iterations = 100;
fun = zeros(1, 100);
fun(1) = 3;
for k=2:iterations
    b = 2 + k / 100;
    delta = max([0; delta - 1 / b]);
    x_0_hold = min([x1 - 1 + delta; x2 - 1 + delta; x0]); 
    x0 = max([1 - delta - x1; 1 - delta - x2 ; x_0_hold]);
    x1 = max([1 - delta + x0; 1 - delta - x0; x1 - (0.002 * x1) / b]);
    x2 = max([1 - delta + x0; 1 - delta - x0; x2 - (0.002 * x2) / b]);
    fun(k) = delta + 0.001 * (x0 + x1 + x2) ^ 0.5;
end
plot(fun(1:iterations))
title("Objective Fn against iterations")
ylim([-0.5 3.5])










% Explored Gradient Method
clc
clear

a1 = [1;0;1];
a2 = [0;1;1];
a3 = [-1;0;1];
a4 = [0;-1;1];

iterations = 1000;
alpha = 0.1;

x = zeros(3, iterations);
x(1,:) = 0;
eu_norm = zeros(1, iterations);
neg_log = zeros(1, iterations);

for K=1:iterations
    gradient = exp(-a1' * x(:,K)) / (1 + exp(-a1' * x(:,K))) * (-a1) + ...
        exp(-a2' * x(:,K)) / (1 + exp(-a2' * x(:,K))) * (-a2) + ...
        exp(a3' * x(:,K)) / (1 + exp(a3' * x(:,K))) * (a3) + ...
        exp(a4' * x(:,K)) / (1 + exp(a4' * x(:,K))) * (a4);
    x(:,K+1) = x(:,K) - alpha * gradient;
    eu_norm(K) = norm(x(:,K));
    neg_log(K) = log(1 + exp(-a1' * x(:,K))) + log(1 + exp(-a2' * x(:,K))) + ...
        1 + exp(a3' * x(:,K)) + 1 + exp(a4' * x(:,K));
    if (norm(gradient) < 0.001)
        break;
    end 
end

figure(1)
plot(eu_norm(1:iterations));
title({'Gradient Descent with step size 0.1 and x_0 as 0'});
ylabel('Euclidean Norm')

figure(2)
plot(neg_log(1:iterations));
title({'Gradient Descent with step size 0.1 and x_0 as 0'});
ylabel('Value of Neg log-likelihood')







% Solver to solve its dual problem
clc
clear


df = 0.9;
cvx_begin
    variable x(9) % variable names and the dimension of each
    minimize(0.5 * x(3)+ 0.4 * x(4) + x(1)) % maximize or minimize the given objective function
    subject to % list constraints
        x(8) + x(9) == 1;
        x(6) + x(7) - df * x(8) == 1;
        x(4) + x(5) - df * x(6) - df * 0.5 * x(9) == 1;
        x(2) + x(3) - df * x(4) - df * 0.5 * x(7) - df * 0.25 * x(9) == 1;
        x(1) - df * x(2) - df * 0.5 * x(5) - df * 0.25 * x(7) - df * 0.125 * x(9) == 1;
        x(2) >= 0;
        x(3) >= 0;
        x(4) >= 0;
        x(5) >= 0;
        x(6) >= 0;
        x(7) >= 0;
        x(8) >= 0;
        x(9) >= 0;
cvx_end










x_0 = 3;
[x1, g1, stopping_iter] = newton_opt(x_0,1000);
figure(1)
plot(x1(1:stopping_iter));
title({'N Method for Optimization', ...
    'for x_0 = 3'});
ylabel('x_k')

figure(2)
plot(g1(1:stopping_iter));
title({'N Method for Optimization', ...
    'for x_0 = 3'});
ylabel('g(x_k)')

x_1 = -0.3;
[x2, g2, stopping_iter] = newton_opt(x_1,1000);
figure(3)
plot(x2(1:stopping_iter));
title({'N Method for Optimization', ...
    'for x_0 = -0.3'});
ylabel('x_k')

figure(4)
plot(g2(1:stopping_iter));
title({'N Method for Optimization', ...
    'for x_0 = -0.3'});
ylabel('g(x_k)')



function [x_vec, g_vec, iteration]=newton_opt(x_0, max_iter)
    f = @(x) (x - 1) / ((x - 2)^2);
    f_1st = @(x) -x / ((x - 2)^3);
    f_2nd = @(x) (2 * x + 2) / ((x - 2)^4);
    d = @(x) -f_1st(x)/f_2nd(x);
    %Initalize containers
    x_new = x_0;
    g_new = f(x_0);
    
    x_vec = zeros(max_iter,1);
    g_vec = zeros(max_iter,1);
    
    x_vec(1)=x_0;
    g_vec(1)=g_new;
    
    iteration=0;
    delta_y=100;
    
    while abs(delta_y)>0.001 && iteration<max_iter
        % transit values
        x_old=x_new;
        g_old=g_new;
        
        % update step
        x_new=x_old+d(x_old);
        
        % book-keeping
        iteration=iteration+1;
        x_vec(iteration)=x_new;
        g_new=f(x_new);
        g_vec(iteration)=g_new;
        
        % calculate the improvement
        delta_y=g_new-g_old;
        fprintf('Iter %d: x=%f, g=%f\n', iteration, x_new,g_new)
    end 
    fprintf('Exit at iter %d.\n', iteration)
end

